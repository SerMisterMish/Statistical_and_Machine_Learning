\documentclass[notheorems, handout]{beamer}

\usetheme{Warsaw}
\setbeamertemplate{page number in head/foot}[totalframenumber]
\setbeamertemplate{headline}{}
\setbeamertemplate{navigation symbols}{}
\usefonttheme[onlymath]{serif}

\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[russian]{babel}

\usepackage{graphicx,subcaption,ragged2e}
\usepackage{tikz}
\usepackage{bm}

\newtheorem{remark}{Замечание}

\title[Статистическое и машинное обучение]{Обучение с учителем}
\institute[Санкт-Петербургский Государственный Университет]{%
  \small
  Санкт-Петербургский государственный университет\\
  Кафедра статистического моделирования
}
\date{16 сентября 2025, Санкт-Петербург}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Введение}
  \textbf{Машинное обучение}~--- это раздел искусственного
  интеллекта, в котором разрабатываются методы и алгоритмы,
  позволяющие компьютерам обнаруживать закономерности в данных и
  делать прогнозы.\medskip

  \textbf{Обучение с учителем}~--- один из способов машинного
  обучения, в ходе которого для каждого наблюдения в обучающем наборе
  известно, какой результат является правильным.\medskip

  \textbf{Пример задач}:
  \begin{itemize}
    \item Регрессия: предсказание стоимости недвижимости, количества
      продаж некоторого товара, погоды.
    \item Классификация: предсказание ценовой категории товара, типа
      изображения, болеет ли человек или нет.
  \end{itemize}
\end{frame}

\begin{frame}{Постановка задачи}
  \textbf{Дано}:
  \begin{enumerate}
    \item Пространство объектов $\mathbb{X}$~--- множество описаний
      объектов (например, фотографии, тексты, таблицы с признаками).
    \item Пространство ответов $\mathbb{Y}$~--- множество меток или
      значений, которые нужно предсказывать (например, классы
      <<кот>>/<<собака>>, цена товара).
    \item Доступная выборка $D = \{(x_i, y_i)\}_{i=1}^n$, где $x_i\in
      \mathbb{X}$, $y_i\in \mathbb{Y}$.
  \end{enumerate}
  \textbf{Модель}:
  \[
    y=f(x) + \varepsilon,
  \]
  где $f(x)$~--- некоторая фиксированная (но неизвестная) функция,
  $\varepsilon$~--- шум, $\mathsf{E}\varepsilon=0$ и $\varepsilon$ не
  зависит от $x$.\medskip

  \textbf{Предположение}: $f(x)$ лежит в некотором классе функций
  (например, в классе линейных функций).\medskip

  \textbf{Задача}: по выборке $D$ построить оценку $\hat f(x)$
  функции $f(x)$ в выбранном классе функций.
\end{frame}

\begin{frame}{Функция потерь и ее минимизация}
  Чтобы оценить, насколько хорошо модель предсказывает ответы,
  используется {\bf функция потерь} для набота данных $L(Y,
  \hat{Y})$. Она показывает, насколько велико расхождение между
  истинными значениями $Y$ и его предсказаниями $\hat{Y} = \hat{f}
  (X)$. Такая функция потерь обычно состоит из {\bf функций потерь
  для одного индивида} $l(y, \hat{y})$, то есть это может быть сумма,
  максимум и т.д. по всем таким функциям.\smallskip

  Тогда задача машинного обучения~--- минимизация выбранной функции потерь:
  \[
    L(Y, \hat Y)\longrightarrow \min.
  \]\smallskip

  В большинстве случаев вычислить точку минимума функции потерь
  аналитически не представляется возможным, поэтому для его
  нахождения прибегают к методам \textbf{детерменированной и
  стохастической оптимизации} (например, перебор значений по сетке,
    метод Ньютона и квазиньютоновские методы, (стохастический)
  градиентный спуск, случайный поиск).
\end{frame}

\begin{frame}{Градиентный спуск}
  Градиентный спуск является наиболее распространенным алгоритмом
  оптимизации в машинном обучении.\medskip

  Пусть $g$~--- некоторая гладкая функция, у которой необходимо найти
  минимум. Обозначим $p_n=-\nabla g(x_n)$~--- направление
  антиградиента в точке $x_n$. Тогда
  \[
    x_{n+1}=x_n+\alpha p_n,
  \]
  где $\alpha$~--- гиперпараметр, отвечающий за скорость обучения.\medskip

  \textbf{Условия сходимости}: выпуклость $g$, липшицевость $\nabla
  g$, ...\medskip

  \textbf{Критерий остановки}: достижение определенного числа
  итераций, малая норма градиента, малое изменение значения функции.
\end{frame}

\begin{frame}{Модификации градиентного спуска}
  В машинном обучении минимизируем функцию потерь:
  \[
    g(x) = L(Y, \hat{Y} (x)) = \frac{1}{n} \sum _{i = 1} ^n l _i (y
    _i, \hat{y} _i (x)).
  \]\medskip

  {\bf Варианты градиентного спуска:}
  \begin{enumerate}
    \item (Batch) Gradient Descent (GD):
      \[
        p_n=-\nabla g(x_n)=-\sum_{i=1}^n \nabla l_i(x_n).
      \]
    \item Mini-batch GD: случайным образом выбирается \textsf{m}
      наблюдений и делается шаг с $p_n=-\sum_{i=1}^m \nabla
      g_{j_i}(x_n)$.\smallskip
    \item Stochastic GD: mini-batch GD с $m=1$.\smallskip
    \item Adam (Adaptive Moment Estimation): основан на GD, каждый
      параметр модели имеет собственную адаптивную скорость обучения,
      основанную на прошлых градиентах.
  \end{enumerate}
\end{frame}

\begin{frame}{Преимущества стохастического градиентного спуска}
  Использование стохастического градиентного спуска вместо обычного
  обусловлено следующими преимущетвами:
  \begin{enumerate}
    \item Меньшее использование памяти: не храним промежуточные
      вычисления для всей выборки;
    \item Обработка онлайн: не надо хранить сразу всё для обучения,
      можно добавлять данные по мере поступления;
    \item Быстрота: одно обновление весов происходит значительно
      быстрее, за меньшее количество эпох можем сойтись;
    \item Регуляризация: не идём сразу в минимум, что может привести
      к переобучению.
  \end{enumerate}

  \begin{center}
    \includegraphics[width=0.75\textwidth]{img/stoch_grad_des.jpg}
  \end{center}
\end{frame}

\begin{frame}{Процесс обучения}
  Процесс обучения любого алгоритма машинного обучения выглядит
  следующим образом:
  \begin{enumerate}
    \item Выборка $D$ предварительно случайным образом разбивается на
      тренировочную и тестовую: $D=D_\text{train} \sqcup
      D_\text{test}$ (чтобы избежать закономерности, заложенной в
      порядок данных).\medskip
    \item На тренировочных данных модель обучается: минимизируется
      выбранная функция потерь $L(y, \hat y)$.\medskip
    \item Если в модели есть гиперпараметр, то выделяют валидационную
      выборку $D_\text{val}$ для подбора гиперпараметров модели или
      производится остановка оптимизации функции потерь.
  \end{enumerate}
\end{frame}

\begin{frame}{Оценка качества модели}
  После обучения нужно оценить качество/обобщающую способность
  модели. Для этого на тестовом множестве вычисляются различные
  метрики. Выбираются они в зависимости от задачи.\smallskip

  Важно понимать разницу между функцией потерь и метрикой качества:
  \begin{itemize}
    \item Функция потерь возникает в тот момент, когда мы сводим
      задачу построения модели к задаче оптимизации.
    \item Метрика — внешний, объективный критерий качества, обычно
      зависящий не от параметров модели, а только от предсказанных меток.
  \end{itemize}
  \begin{table}[h!]
    \centering
    \caption{Метрики для задач регрессии и классификации}
    \begin{tabular}{|p{3cm}|p{6cm}|}
      \hline
      Регрессия & MSE, RMSE, MAE, MAPE, WAPE \\
      \hline
      Классификация & Accuracy, Precision, Recall, F1-score, ROC AUC, PR AUC \\
      \hline
    \end{tabular}
  \end{table}\medskip
\end{frame}

\begin{frame}{Линейная классификация}
  Пусть целевая переменная $y$ принимает значения $\{-1, 1\}$. Хотим
  обучить модель так, чтобы плоскость, которую она задает, как можно
  лучше отделяла объекты одного класса от другого.\medskip

  Линейный классификатор:
  \[
    \hat y = \hat f(x; w) = \operatorname{sign} \langle x, w\rangle.
  \]
  Функция потерь:
  \[
    L(y, \hat{y}) = \sum_{i=1}^{n}\mathbb{I}[y_i \langle x_i,
    w\rangle < 0] \longrightarrow \min_{w}.
  \]
\end{frame}

\begin{frame}{Линейная классификация. Отступ}
  Величина $M_i=y_i \langle x_i, w\rangle$ называется
  \textbf{отступом} (margin) классификатора. Абсолютная величина
  отступа говорит о степени уверенности классификатора.\medskip

  \textbf{Проблема}: функция $\mathbb{I}[M < 0]$ кусочно-постоянная,
  следовательно функцию потерь невозможно оптимизировать методами,
  основанными на производных, поскольку во всех точках производная
  равна нулю.\medskip

  \textbf{Решение}: можно мажорировать эту функцию более гладкой
  функцией и минимизировать функцию потерь с этой мажорирующей
  функцией с помощью методов численной оптимизации.
\end{frame}

\begin{frame}{Линейная классификация. Функции потерь}
  \begin{enumerate}
    \item Перцептрон: $L(M) = \max(0, -M)$~--- отступы учитываются
      только для неправильно классифицированных объектах
      пропорционально величине отступа.
    \item Hinge (SVM): $L(M) = \max(0, 1-M)$~--- объекты, которые
      классифицированы правильно, но не очень <<уверенно>>,
      продолжают вносить свой вклад в градиент.
    \item Логистическая: $L(M) = \ln\left(1+e^{-M}\right)$.
  \end{enumerate}
  \begin{figure}
    \includegraphics[width=0.75\textwidth]{img/loss_major.pdf}
  \end{figure}
\end{frame}

\begin{frame}{Логистическая регрессия}
  Посмотрим на задачу классификации как на задачу предсказания
  вероятностей и введём следующую модель: есть зависимая переменная
  $y$, принимающая значения только $0$ и $1$:
  \[
    P (y = 1|x) = p = h(z),
  \]
  где $h(z) \in [0, 1]$, $z = \langle x, w\rangle$ ~--- линейная
  регрессия. Преобразование $h$, которое нам подходит~--- сигмоида:
  \[
    p_i = \frac{1}{1+e^{-\langle x_i, w\rangle}}=\sigma(\langle x_i, w\rangle).
  \]
  Функция правдоподобия для Бернулли ($y \in \{0, 1\}$):
  \[
    p(y~|~\mathbf{X}, w)=\prod_{i=1}^np_i^{y_i}(1-p_i)^{1-y_i}.
  \]
  Прологарифмируем:
  \[
    \sum_{i=1}^n\Big[y_i\ln(\sigma(\langle x_i, w\rangle)) +
    (1-y_i)\ln(1-\sigma(\langle x_i, w\rangle))\Big].
  \]
\end{frame}

\begin{frame}{Логистическая регрессия. Связь с отступом}
  Теперь пусть $y\in\{-1, 1\}$. Тогда, поскольку
  $\sigma(z)=1-\sigma(-z)$, логарифм правдоподобия можно представить
  в следующем виде:
  \begin{align*}
    \ln p (y~|~ \mathbf{X}, w) & =
    -\sum_{i=1}^n\Big[\mathbb{I}[y_i=1]\sigma(z_i)+\mathbb{I}[y_i=-1]\left(1-\sigma(z_i)\right)\Big]
    \\
    & = -\sum_{i=1}^n \ln\sigma(y_i\langle x_i, w\rangle)
    \\
    & = \sum_{i=1}^n \ln\left(1 + e^{-M}\right)
  \end{align*}

  Таким образом, функцию потерь в логистической регрессии можно
  представить в виде функции от отступа.
\end{frame}

\begin{frame}{Метод опорных векторов (SVM): Основная идея}
  \framesubtitle{Геометрическая интуиция}

  \textbf{Цель}: Найти не просто разделяющую гиперплоскость, а ту,
  которая \alert{максимизирует зазор (отступ)} между классами.

  \begin{columns}
    \begin{column}{0.5\textwidth}
      \begin{itemize}
        \item Гиперплоскость: $\mathbf{w}^T \mathbf{x} + b = 0$
        \item \alert{Отступ (Margin)} — расстояние до ближайших точек классов.
        \item \alert{Опорные векторы} — точки, лежащие на границах
          отступа. Именно они <<определяют>> положение гиперплоскости.
      \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
      \includegraphics[width=\textwidth]{img/svm_margin.jpg}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Метод опорных векторов (SVM): Основная идея}
  \framesubtitle{Постановка задачи}

  \textbf{Задача оптимизации} (для линейно разделимых данных):
  \[
    \begin{cases}
      \min_{\mathbf{w}, b} \frac{1}{2} ||\mathbf{w}||^2 \\
      \text{при условии } y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1,
      \quad i = 1, \dots, n
    \end{cases}
  \]
  Минимизируем норму вектора весов $\Rightarrow$ \alert{максимизируем
  зазор} $\gamma = \frac{2}{||\mathbf{w}||}$.

  Это задача квадратичной оптимизации, которая решается путём
  составления двойственной задачи (по теореме Куна-Таккера).
\end{frame}

\begin{frame}{SVM для линейно неразделимых данных}
  \framesubtitle{Мягкий зазор (Soft Margin)}

  \begin{itemize}
    \item В реальных данных идеальная линейная разделимость — редкость.
    \item Вводятся \alert{ослабляющие переменные (slack variables)}
      $\xi_i \geq 0$.
    \item Они позволяют точкам нарушать границу отступа.
    \item $\xi_i$ — штраф за неправильную классификацию или
      нахождение в полосе зазора.
  \end{itemize}
  \begin{center}
    \includegraphics[width=0.7\textwidth]{img/soft_margin.png}
  \end{center}
\end{frame}

\begin{frame}{SVM для линейно неразделимых данных}
  \framesubtitle{Постановка задачи}
  \textbf{Новая задача оптимизации (Soft Margin SVM)}:
  \[
    \begin{cases}
      & \min_{\mathbf{w}, b, \xi_i} \frac{1}{2} ||\mathbf{w}||^2 + C
      \sum_{i=1}^n \xi_i \\
      & \text{при условии:} \\
      & \quad y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1 - \xi_i,
      \quad i = 1, \dots, n \\
      & \quad \xi_i \geq 0, \quad i = 1, \dots, n
    \end{cases}
  \]

  \alert{Параметр $C$} управляет компромиссом:
  \begin{itemize}
    \item \textbf{Большой $C$}: Больший штраф за ошибки $\Rightarrow$
      уже разделение, риск переобучения.
    \item \textbf{Малый $C$}: Меньший штраф за ошибки $\Rightarrow$
      шире зазор, больше обобщающая способность.
  \end{itemize}
\end{frame}

\begin{frame}{Сравнение с логистической регрессией и Ядра}
  \framesubtitle{От меток к отступу; От линейности к нелинейности}

  \begin{block}{SVM vs. Логистическая регрессия}
    \begin{itemize}
      \item \textbf{Логистическая регрессия}: Строит
        \alert{вероятностную} модель $P(y=1|\mathbf{x})$.
        Минимизирует \alert{логистическую функцию потерь} по всем
        объектам. Все точки влияют на решение.
      \item \textbf{SVM}: Строит \alert{разделяющую гиперплоскость}.
        Фокусируется на \alert{максимизации отступа}; решение зависит
        только от \alert{опорных векторов}. Более устойчив к выбросам.
    \end{itemize}
  \end{block}

  \begin{block}{Нелинейность: Kernel Trick}
    Что если данные нелинейно разделимы?

    Идея: Отображаем данные в пространство \alert{большей
    размерности} ($\phi(\mathbf{x}): \mathbb{R}^p \rightarrow
    \mathbb{R}^m$), где они становятся линейно разделимыми.

    \alert{Ядро (Kernel)}: $K(\mathbf{x}_i, \mathbf{x}_j) = \langle
    \phi(\mathbf{x}_i), \phi(\mathbf{x}_j) \rangle$. Позволяет
    работать в высокомерном пространстве \alert{без явного
    вычисления} $\phi(\mathbf{x})$.
  \end{block}
\end{frame}

\begin{frame}{Линейные и нелинейные ядра}
  \framesubtitle{Практическое применение}
  \small

  \begin{columns}
    \begin{column}{0.5\textwidth}
      \textbf{Линейное ядро}
      \vspace{-3mm}
      \[
        K(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{x}_i^T \mathbf{x}_j
      \]
      \vspace{-6mm}
      \begin{itemize}
        \item Исходное признаковое пространство.
        \item Быстрое обучение.
      \end{itemize}
      \includegraphics[width=\textwidth]{img/linear_kernal.jpg}
    \end{column}
    \begin{column}{0.5\textwidth}
      \textbf{Нелинейные ядра}
      \vspace{-3mm}
      \begin{itemize}
        \item \textbf{RBF (Gaussian)}:
          $K(\mathbf{x}_i, \mathbf{x}_j) = \exp(-\gamma
          ||\mathbf{x}_i - \mathbf{x}_j||^2)$
        \item \textbf{Полиномиальное}:
          $K(\mathbf{x}_i, \mathbf{x}_j) = (\gamma \cdot
          \mathbf{x}_i^T \mathbf{x}_j + r)^d$
      \end{itemize}
      Гибкие сложные границы решений. Требуют подбора параметров
      ($\gamma$, $d$).
      \includegraphics[width=\textwidth]{img/rbf_kernel}
    \end{column}
  \end{columns}
\end{frame}

\end{document}