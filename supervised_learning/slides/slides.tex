\documentclass[notheorems, handout]{beamer}

\usetheme{Warsaw}
\setbeamertemplate{page number in head/foot}[totalframenumber]
\setbeamertemplate{headline}{}
\setbeamertemplate{navigation symbols}{}
\usefonttheme[onlymath]{serif}

\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[russian]{babel}

\usepackage{graphicx,subcaption,ragged2e}
\usepackage{tikz}
\usepackage{bm}

\newtheorem{remark}{Замечание}

\title[Статистическое и машинное обучение]{Обучение с учителем}
\institute[Санкт-Петербургский Государственный Университет]{%
	\small
	Санкт-Петербургский государственный университет\\
	Кафедра статистического моделирования
}
\date{16 сентября 2025, Санкт-Петербург}

\begin{document}

\begin{frame}
	\titlepage
\end{frame}

\begin{frame}{Введение}
	\textbf{Машинное обучение}~--- это раздел искусственного интеллекта, в котором разрабатываются методы и алгоритмы, позволяющие компьютерам обнаруживать закономерности в данных и делать прогнозы без явных инструкций.\medskip

	\textbf{Обучение с учителем}~--- один из способов машинного обучения, в ходе которого для каждого примера в обучающем наборе известно, какой результат является правильным.\medskip

	\textbf{Пример задач}:
	\begin{itemize}
		\item Регрессия: предсказание стоимости недвижимости, количества продаж некоторого товара, погоды.
		\item Классификация: предсказание ценовой категории товара, типа изображения, болеет ли человек или нет.
	\end{itemize}
\end{frame}

\begin{frame}{Постановка задачи}
	\textbf{Дано}:
	\begin{enumerate}
		\item Пространство объектов $X$~--- множество описаний объектов (например, фотографии, тексты, таблицы с признаками).
		\item Пространство ответов $Y$~--- множество меток или значений, которые нужно предсказывать (например, классы <<кот>>/<<собака>>, цена товара).
		\item Обучающая выборка $D = \{(x_i, y_i)\}_{i=1}^n$, где $x_i\in X$, $y_i\in Y$.
	\end{enumerate}
	\textbf{Модель}:
	\[
		y=f(x) + \varepsilon,
	\]
	где $f(x)$~--- некоторая фиксированная (но неизвестная) функция, $\varepsilon$~--- шум, $\mathsf{E}\varepsilon=0$ и $\varepsilon$ не зависит от $x$.\medskip
	% \]
	% \[
	% a:X\longrightarrow Y,
	% \]
	% сопоставляющая набору признаков значение целевой переменной.\medskip

	\textbf{Предположение}: $f(x)$ лежит в некотором классе функций (например, в классе линейных функций).\medskip

	\textbf{Задача}: по обучающей выборке $D$ построить оценку $\hat f(x)$ функции $f(x)$ в выбранном классе функций.
	% чтобы ее предсказания $\hat y=\hat a(x)$ были как можно ближе к истинным ответам $y$.
\end{frame}

\begin{frame}{Функция потерь и ее минимизация}
	Чтобы оценить, насколько хорошо модель предсказывает ответы, используется {\bf функция потерь} $L(y, \hat{y})$. Она показывает, насколько велико расхождение между истинными значениями $y$ и его предсказаниями $\hat{y}$.\bigskip

	Тогда задача машинного обучения~--- минимизация выбранной функции потерь:
	\[
		L(y, \hat y)\longrightarrow \min.
	\]\smallskip

	В большинстве случаев вычислить точку минимума функции потерь аналитически не представляется возможным, поэтому для его нахождения прибегают к методам \textbf{детерменированной и стохастической оптимизации} (например, перебор значений по сетке, метод Ньютона и квазиньютоновские методы, (стохастический) градиентный спуск, случайный поиск).

	% {\bf Примеры}:
	% \begin{itemize}
	% 	\item Для задачи \textbf{регрессии} наиболее распространенной функцией потерь является среднеквадратичная ошибка (MSE):
	% 	      \[
	% 		      L(y, \hat{y}) = \frac{1}{n}\sum_{i=1}^n (y_i - \hat y_i)^2.
	% 	      \]
	% 	\item Для задачи \textbf{бинарной классификации}, если $\hat y$ представляет собой вектор вероятностей принадлежности к положительному классу, используется кросс-энтропия:
	% 	      \[
	% 		      L(y, \hat{y}) = -\sum_{i=1}^n\left[y_i \ln\hat y_i + (1 - y_i)\ln (1 - \hat y_i)\right].
	% 	      \]
	% \end{itemize}
\end{frame}

\begin{frame}{Градиентный спуск}
	Градиентный спуск является наиболее распространенным алгоритмом оптимизации в машинном обучении.\medskip

	Пусть $f$~--- некоторая гладкая функция, у которой необходимо найти минимум. Обозначим $p_n=-\nabla f(x_n)$~--- направление антиградиента в точке $x_n$. Тогда
	\[
		x_{n+1}=x_n+\alpha p_n,
	\]
	где $\alpha$~--- гиперпараметр, отвечающий за скорость обучения.\medskip

	\textbf{Условия сходимости}: выпуклость $f$, липшицевость $\nabla f$, ...\medskip

	\textbf{Критерий остановки}: достижение определенного числа итераций, малая норма градиента, малое изменение значения функции.
\end{frame}

\begin{frame}{Модификации градиентного спуска}
	В машинном обучении:
	\[
		f(x)=\frac{1}{n}\sum_{i=1}^n f_i(x),
	\]
	где $f_i$~--- функция потерь для $i$-го наблюдения.\medskip
	\begin{enumerate}
		\item (Batch) Gradient Descent (GD):
		      \[
			      p_n=-\nabla f(x_n)=-\sum_{i=1}^n \nabla f_i(x_n).
		      \]
		\item Mini-batch GD: случайным образом выбирается \textsf{m} наблюдений и делается шаг с $p_n=-\sum_{i=1}^m \nabla f_{j_i}(x_n)$.\smallskip
		\item Stochastic GD: mini-batch GD с $m=1$.\smallskip
		\item Adam (Adaptive Moment Estimation): основан на GD, каждый параметр модели имеет собственную адаптивную скорость обучения, основанную на прошлых градиентах.
	\end{enumerate}
\end{frame}

\begin{frame}{Процесс обучения}
	Процесс обучения любого алгоритма машинного обучения выглядит следующим образом:
	\begin{enumerate}
		\item Выборка $D$ предварительно разбивается на тренировочную и тестовую: $D=D_\text{train} \sqcup D_\text{test}$ .\medskip
		\item На тренировочных данных модель обучается: минимизируется выбранная функция потерь $L(y, \hat y)$.\medskip
		\item Также часто присутствует и валидационная выборка $D_\text{val}$, на основе которой подбираются гиперпараметры модели/производится остановка оптимизации функции потерь.
	\end{enumerate}
\end{frame}

\begin{frame}{Оценка качества модели}
	После обучения нужно оценить качество/обобщающую способность модели. Для этого на тестовом множестве вычисляются различные метрики. Выбираются они в зависимости от задачи.\smallskip

	Важно понимать разницу между функцией потерь и метрикой качества:
	\begin{itemize}
		\item Функция потерь возникает в тот момент, когда мы сводим задачу построения модели к задаче оптимизации.
		\item Метрика — внешний, объективный критерий качества, обычно зависящий не от параметров модели, а только от предсказанных меток.
	\end{itemize}
	\begin{table}[h!]
		\centering
		\caption{Метрики для задач регрессии и классификации}
		\begin{tabular}{|p{3cm}|p{6cm}|}
			\hline
			Регрессия & MSE, RMSE, MAE, MAPE, WAPE \\
			\hline
			Классификация & Accuracy, Precision, Recall, F1-score, ROC AUC, PR AUC \\
			\hline
		\end{tabular}
	\end{table}\medskip
\end{frame}

\begin{frame}{Линейная классификация}
	Пусть целевая переменная $y$ принимает значения $\{-1, 1\}$. Хотим обучить линейную модель так, чтобы плоскость, которую она задает, как можно лучше отделяла объекты одного класса от другого.\medskip

	Линейный классификатор:
	\[
		\hat y = \hat f(x; w) = \operatorname{sign} \langle x, w\rangle.
	\]
	Функция потерь:
	\[
		L(y, \hat{y}) = \sum_{i=1}^{n}\mathbb{I}[y_i \langle x_i, w\rangle < 0] \longrightarrow \min_{w}.
	\]
\end{frame}

\begin{frame}{Линейная классификация. Отступ}
	Величина $M_i=y_i \langle x_i, w\rangle$ называется \textbf{отступом} (margin) классификатора. Абсолютная величина отступа говорит о степени уверенности классификатора.\medskip

	\textbf{Проблема}: функция $\mathbb{I}[M < 0]$ кусочно-постоянная, следовательно функцию потерь невозможно оптимизировать градиентными методами, поскольку во всех точках производная равна нулю.\medskip

	\textbf{Решение}: можно мажорировать эту функцию более гладкой функцией и минимизировать функцию потерь с этой мажорирующей функцией с помощью методов численной оптимизации.
\end{frame}

\begin{frame}{Линейная классификация. Функции потерь}
	\begin{enumerate}
		\item Перцептрон: $L(M) = \max(0, -M)$~--- отступы учитываются только для неправильно классифицированных объектах пропорционально величине отступа.
		\item Hinge (SVM): $L(M) = \max(0, 1-M)$~--- объекты, которые классифицированы правильно, но не очень <<уверенно>>, продолжают вносить свой вклад в градиент.
		\item Логистическая: $L(M) = \ln\left(1+e^{-M}\right)$.
	\end{enumerate}
	\begin{figure}
		\includegraphics[width=0.75\textwidth]{img/loss_major.pdf}
	\end{figure}
\end{frame}

\begin{frame}{Логистическая регрессия}
	Посмотрим на задачу классификации как на задачу предсказания вероятностей (например, предсказание <<кликабельности>> рекламного баннера).

	\textbf{Принцип работы}: научить линейную модель предсказывать значения $z\in\mathbb{R}$ (логиты), а затем преобразовывать их в вероятности с помощью сигмоиды:
	\[
		z_i=\langle x_i, w\rangle = \ln\frac{p_i}{1 - p_i},\quad p_i = \frac{1}{1+e^{-\langle x_i, w\rangle}}=\sigma(\langle x_i, w\rangle).
	\]
	Функция правдоподобия для распределения Бернулли:
	\[
		p(y~|~\mathbf{X}, w)=\prod_{i=1}^np_i^{y_i}(1-p_i)^{1-y_i}.
	\]
	Прологарифмируем:
	\[
		\sum_{i=1}^n\Big[y_i\ln(\sigma(\langle x_i, w\rangle)) + (1-y_i)\ln(1-\sigma(\langle x_i, w\rangle))\Big].
	\]
\end{frame}

\begin{frame}{Логистическая регрессия. Связь с отступом}
	Теперь пусть $y\in\{-1, 1\}$. Тогда, поскольку $\sigma(z)=1-\sigma(-z)$, логарифм правдоподобия можно представить в следующем виде:
	\begin{align*}
		\ln p (y~|~ \mathbf{X}, w) & = -\sum_{i=1}^n\Big[\mathbb{I}[y_i=1]\sigma(z_i)+\mathbb{I}[y_i=-1]\left(1-\sigma(z_i)\right)\Big] \\
		                           & = -\sum_{i=1}^n \ln\sigma(y_i\langle x_i, w\rangle)                                                \\
		                           & = \sum_{i=1}^n \ln\left(1 + e^{-M}\right)
	\end{align*}

	Таким образом, функцию потерь в логистической регрессии можно представить в виде функции от отступа.
\end{frame}

\begin{frame}{Метод опорных векторов (SVM): Основная идея}
	\framesubtitle{Геометрическая интуиция}

	\textbf{Цель}: Найти не просто разделяющую гиперплоскость, а ту, которая \alert{максимизирует зазор (отступ)} между классами.

	\begin{columns}
		\begin{column}{0.5\textwidth}
			\begin{itemize}
				\item Гиперплоскость: $\mathbf{w}^T \mathbf{x} + b = 0$
				\item \alert{Отступ (Margin)} — расстояние до ближайших точек классов.
				\item \alert{Опорные векторы} — точки, лежащие на границах отступа. Именно они <<определяют>> положение гиперплоскости.
			\end{itemize}
		\end{column}
		\begin{column}{0.5\textwidth}
			\includegraphics[width=\textwidth]{img/svm_margin.jpg}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}{Метод опорных векторов (SVM): Основная идея}
	\framesubtitle{Постановка задачи}

	\textbf{Задача оптимизации} (для линейно разделимых данных):
	\[
	\begin{cases}
		\min_{\mathbf{w}, b} \frac{1}{2} ||\mathbf{w}||^2 \\
		\text{при условии } y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1, \quad i = 1, \dots, n
	\end{cases}
	\]
	Минимизируем норму вектора весов $\Rightarrow$ \alert{максимизируем зазор} $\gamma = \frac{2}{||\mathbf{w}||}$.

	Это задача квадратичной оптимизации, которая решается путём составления двойственной задачи.
\end{frame}

\begin{frame}{SVM для линейно неразделимых данных}
	\framesubtitle{Мягкий зазор (Soft Margin)}

	\begin{itemize}
		\item В реальных данных идеальная линейная разделимость — редкость.
		\item Вводятся \alert{ослабляющие переменные (slack variables)} $\xi_i \geq 0$.
		\item Они позволяют точкам нарушать границу отступа.
		\item $\xi_i$ — штраф за неправильную классификацию или нахождение в полосе зазора.
	\end{itemize}
	\begin{center}
		\includegraphics[width=0.7\textwidth]{img/soft_margin.png}
	\end{center}
\end{frame}

\begin{frame}{SVM для линейно неразделимых данных}
	\framesubtitle{Постановка задачи}
	\textbf{Новая задача оптимизации (Soft Margin SVM)}:
	\[
	\begin{cases}
		& \min_{\mathbf{w}, b, \xi_i} \frac{1}{2} ||\mathbf{w}||^2 + C \sum_{i=1}^n \xi_i \\
		& \text{при условии:} \\
		& \quad y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1 - \xi_i, \quad i = 1, \dots, n \\
		& \quad \xi_i \geq 0, \quad i = 1, \dots, n
	\end{cases}
	\]

	\alert{Параметр $C$} управляет компромиссом:
	\begin{itemize}
		\item \textbf{Большой $C$}: Больший штраф за ошибки $\Rightarrow$ уже разделение, риск переобучения.
		\item \textbf{Малый $C$}: Меньший штраф за ошибки $\Rightarrow$ шире зазор, больше обобщающая способность.
	\end{itemize}
\end{frame}

\begin{frame}{Сравнение с логистической регрессией и Ядра}
	\framesubtitle{От меток к отступу; От линейности к нелинейности}

	\begin{block}{SVM vs. Логистическая регрессия}
		\begin{itemize}
			\item \textbf{Логистическая регрессия}: Строит \alert{вероятностную} модель $P(y=1|\mathbf{x})$. Минимизирует \alert{логистическую функцию потерь} по всем объектам. Все точки влияют на решение.
			\item \textbf{SVM}: Строит \alert{разделяющую гиперплоскость}. Фокусируется на \alert{максимизации отступа}; решение зависит только от \alert{опорных векторов}. Более устойчив к выбросам.
		\end{itemize}
	\end{block}

	\begin{block}{Нелинейность: Kernel Trick}
		Что если данные нелинейно разделимы?

		Идея: Отображаем данные в пространство \alert{большей размерности} ($\phi(\mathbf{x}): \mathbb{R}^p \rightarrow \mathbb{R}^m$), где они становятся линейно разделимыми.

		\alert{Ядро (Kernel)}: $K(\mathbf{x}_i, \mathbf{x}_j) = \langle \phi(\mathbf{x}_i), \phi(\mathbf{x}_j) \rangle$. Позволяет работать в высокомерном пространстве \alert{без явного вычисления} $\phi(\mathbf{x})$.
	\end{block}
\end{frame}

\begin{frame}{Линейные и нелинейные ядра}
	\framesubtitle{Практическое применение}
	\small

	\begin{columns}
		\begin{column}{0.5\textwidth}
			\textbf{Линейное ядро}
			\vspace{-3mm}
			\[
			K(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{x}_i^T \mathbf{x}_j
			\]
			\vspace{-6mm}
			\begin{itemize}
				\item Исходное признаковое пространство.
				\item Быстрое обучение.
			\end{itemize}
			\includegraphics[width=\textwidth]{img/linear_kernal.jpg}
		\end{column}
		\begin{column}{0.5\textwidth}
			\textbf{Нелинейные ядра}
			\vspace{-3mm}
			\begin{itemize}
				\item \textbf{RBF (Gaussian)}:
			$K(\mathbf{x}_i, \mathbf{x}_j) = \exp(-\gamma ||\mathbf{x}_i - \mathbf{x}_j||^2)$
			\item \textbf{Полиномиальное}:
			$K(\mathbf{x}_i, \mathbf{x}_j) = (\gamma \cdot \mathbf{x}_i^T \mathbf{x}_j + r)^d$
			\end{itemize}
			Гибкие сложные границы решений. Требуют подбора параметров ($\gamma$, $d$).
			\includegraphics[width=\textwidth]{img/rbf_kernel}
		\end{column}
	\end{columns}
\end{frame}

\end{document}