\documentclass[unicode, notheorems, handout]{beamer}

\usetheme[numbers,totalnumbers,compress, nologo]{Statmod}
\usefonttheme[onlymath]{serif}
\setbeamertemplate{navigation symbols}{}

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, shapes}
\usepackage{ragged2e}
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage{array}

\newtheorem{corollary}{Следствие}
\newtheorem{proposition}{Предложение}
\newtheorem{definition}{Определение}

\title[Трансформеры]{Трансформеры}

\institute[Санкт-Петербургский Государственный Университет]{%
  \small
  Санкт-Петербургский государственный университет\\
  Кафедра статистического моделирования\\
  Семинар <<Статистическое и машинное обучение>>
}

\date[Ноябрь 2025]{Санкт-Петербург, 2025}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Проблемы языковых моделей до появления внимания}
  \footnotesize

  \begin{block}{Модель в идеальном мире}
    \begin{itemize}
      \item \textbf{Effective} --- хорошо справляется с задачей, для
        которой эта модель используется
      \item \textbf{Efficient} --- оптимально потребляет ресурсы с
        точки зрения памяти, CPU и т.д.
    \end{itemize}
  \end{block}

  \begin{alertblock}{Ограничения RNN-архитектур}
    \begin{itemize}
      \item \textbf{Не Effective} --- потеря контекста
        \begin{itemize}
            \scriptsize
          \item Вся информация о последовательности содержится в
            скрытом состоянии, которое обновляется на каждом шаге
          \item Длинные зависимости «забываются» из-за механизма
            обновления состояния
        \end{itemize}

      \item \textbf{Не Efficient} --- невозможность  распараллеливания
        \begin{itemize}
            \scriptsize
          \item Вычисления на шаге $t$ зависят от результатов шага $t-1$
        \end{itemize}
    \end{itemize}
  \end{alertblock}

  \vfill
  \textbf{Вывод:} Требуется механизм прямого доступа к любой части
  последовательности

  \textbf{Решение}: Механизм \textit{Self-Attention} в архитектуре Transformer
\end{frame}

\begin{frame}{Механизм внимания}
  \begin{block}{Основные компоненты}
    \begin{itemize}
        \footnotesize
      \item \textbf{Keys (K)} --- определяет релевантность токена для
        различных запросов
        \[k_{m} = \beta_k + W_k x_m\]
      \item \textbf{Queries (Q)} ---  выражает информационные
        потребности токена в контексте
        \[q_n = \beta_q + W_q x_n\]
      \item \textbf{Values (V)} --- информация, которую мы хотим извлечь
        \[v_m = \beta_v + W_v x_m\]
    \end{itemize}
  \end{block}

  \begin{itemize}
      \scriptsize
    \item $N$ --- длина контекста (количество входных токенов)
    \item $D$ --- размерность векторного представления токена
    \item $D_q$ --- размерность ключей/запросов/значений
    \item $x_m\in \mathbb{R}^D$ --- векторное представление $m$-го токена
    \item $X \in \mathbb{R}^{D \times N}$ --- матрица эмбеддингов
    \item $\beta^* \in \mathbb{R}^{D_q}$, $W^* \in \mathbb{R}^{D_q
      \times D_q}$ --- веса линейного слоя для
      ключей/запросов/значений, общие для всех векторных представлений токенов
    \item $K, V, Q \in \mathbb{R}^{D_q \times N}$ --- матрицы ключей,
      значений и запросов
  \end{itemize}
\end{frame}

\begin{frame}{Dot-product self-attention}
  \begin{enumerate}
      \scriptsize
    \item Каждый токен $x_m$ представляется в виде ключа $k_m$,
      значения $v_m$ и запроса~$q_m$
    \item Для каждой пары $(n, m)$ вычисляется скалярное произведение
      $k_m^T q_n$
    \item Применяется softmax для получения вероятностного
      распределения внимания
    \item Вычисляется взвешенная сумма значений с весами внимания,
      тем самым для каждого токена получаем его
      контекстуализированное представление
  \end{enumerate}
  \vspace{-4.2ex}

  \begin{figure}
    \centering
    \includegraphics[width=1.11\textwidth]{img/dot_prod_att.pdf}
  \end{figure}
\end{frame}

\begin{frame}{Scaled dot-product self-attention}
  \textbf{Проблемы}:
  \begin{itemize}
    \item Большие значения скалярных произведений приводят к малым градиентам
    \item Softmax становится нечувствительным к небольшим изменениям входов
  \end{itemize}
  \vspace{1ex}

  \textbf{Решение} --- масштабирование:
  \[Sa(X) = V \cdot \text{Softmax}\left(\frac{K^T Q}{\sqrt{D_q}}\right)\]

  При условии, что $k_i, q_i \sim \mathcal{N}(0,1)$ и независимы
  $$
  \text{Var}[k^T q] = \text{Var}\left[\sum_{i=1}^{D_q} k_i q_i\right]
  = D_q \;\implies \;
  \text{Var}\left[\frac{k^T q}{\sqrt{D_q}}\right] = 1.$$
\end{frame}

\begin{frame}{Multi-head attention}
  \begin{itemize}
    \item Вместо одного слоя внимания применяется несколько
      параллельных «голов» внимания с разными весами и  размерностью $D/H$
    \item Каждая голова изучает разные типы зависимостей
  \end{itemize}
  \begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{img/mh_att.pdf}
  \end{figure}
\end{frame}

\begin{frame}{Multi-Query Attention}
  \begin{tikzpicture}[
      >=Stealth,
      node/.style={draw, rectangle, minimum width=2cm, minimum
      height=1cm, align=center},
      process/.style={draw, rectangle, rounded corners, minimum
      height=1cm, minimum width=2cm, align=center}
    ]
    \node (kv) at (0, 0) [process] {KV head\\$V = W_v X + \beta_v$\\
    $K = W_k X + \beta_k$};

    \node (q1) at (-3, -2) [process] {Query head
      \#1
      \\$Q_1 = W_{q,1} X + \beta_{q,1} $\\
    $Sa_1(X) = V \cdot\mathrm{Sm}(K^T Q_1)$};

    \node (qh) at (3, -2) [process] {Query head \#H\\$Q_H =
      W_{q,H} X + \beta_{q,H}$\\
    $Sa_H(X) = V \cdot \mathrm{Sm}(K^T Q_H)$};

    \node (dots) at (0, -2) {$\cdots$};

    \node (output) at (0, -4) [process] {Multi-query
      self-attention\\$MhSa(X) = W_c \cdot
    \mathrm{cat}(Sa_1(X), \ldots, Sa_H(X))$};

    \draw[->] (kv) -- (q1);
    \draw[->] (kv) -- (qh);
    \draw[->] (q1) -- (output);
    \draw[->] (qh) -- (output);

  \end{tikzpicture}

  \vspace{0.2cm}

  $\mathrm{Sm} = \mathrm{Softmax}$
\end{frame}

\begin{frame}{Grouped-Query Attention}
  \begin{tikzpicture}[
      >=Stealth,
      node/.style={draw, rectangle, minimum width=2cm, minimum
      height=1cm, align=center},
      process/.style={draw, rectangle, rounded corners, minimum
      height=1cm, minimum width=2cm, align=center}
    ]
    \node (kv1) at (-3, 0) [process] {KV head \#1};
    \node (kvl) at (3, 0) [process] {KV head \#($H / G$)};

    \node (q11) at (-4.5, -2.5) [process] {Query \\head \\\#1};
    \node (q1d) at (-1.5, -2.5) [process] {Query \\head \\\#G};

    \node (qg1) at (1.5, -2.5) [process] {Query \\head \\\#($H - $\\$G + 1$)};
    \node (qgd) at (4.5, -2.5) [process] {Query \\head \\\#($H$)};

    \node (dots0) at (0, 0) {$\cdots$};
    \node (dots1) at (-3, -2.5) {$\cdots$};
    \node (dots2) at (0, -2.5) {\huge $\cdots$};
    \node (dots3) at (3, -2.5) {$\cdots$};

    \node (output) at (0, -5) [process] {Multi-query
      self-attention\\$MhSa(X) = W_c \cdot
    \mathrm{cat}(Sa_1(X), \ldots, Sa_H(X))$};

    \draw[->] (kv1) -- (q11);
    \draw[->] (kv1) -- (q1d);
    \draw[->] (kvl) -- (qg1);
    \draw[->] (kvl) -- (qgd);
    \draw[->] (q11) -- (output);
    \draw[->] (q1d) -- (output);
    \draw[->] (qg1) -- (output);
    \draw[->] (qgd) -- (output);

  \end{tikzpicture}
\end{frame}

\begin{frame}{Позиционное Кодирование (Positional encoding)}
  Применение self-attention теряет информацию о порядке токенов в
  исходной последовательности

  \vspace{0.4cm}

  Идея: добавить информацию о порядке на embedding-этапе

  \begin{enumerate}
    \item Абсолютная позиция: $X \mapsto X + P$, матрица $P$ может
      быть фиксирована:
      \begin{gather*}
        P_{i, 2j} = \sin\left(\frac{i}{10'000^{2j / D}}\right),\\
        P_{i, 2j + 1} = \cos\left(\frac{i}{10'000^{2j / D}}\right),
      \end{gather*}
      а может быть и обучаема

    \item Относительная позиция: на самом деле нас интересует не
      абсолютное расположение токенов, а относительное друг к другу.
  \end{enumerate}
\end{frame}

\begin{frame}{RoPE}
  Есть много вариантов...
  \begin{itemize}
    \item Rotary Position Embedding (RoPE)
      \begin{gather*}
        k_m^\top q_n \mapsto \left\langle k_m, q_n\right\rangle =
        g(x_m, x_n, n-m),\\
        g(x_m, x_n, n - m) = x_n^\top W_q \mathbf{R}^d_{\Theta, n-m} W_kx_m,\\
        \Theta = (\theta_i)_{i=1}^{D/2}, \qquad \theta_i=10'000^{-2(i-1)/D}
      \end{gather*}
  \end{itemize}
  {\scriptsize
    \[
      \hspace*{-0.4cm}
      \mathbf{R}^D_{\Theta,m} =
      \begin{pmatrix}
        \cos{m\theta_1}& -\sin{m\theta_1}&0&0&\cdots&0&0\\
        \sin{m\theta_1}&\cos{m\theta_1}&0&0&\cdots&0&0 \\
        0&0&\cos{m\theta_2}& -\sin{m\theta_2}&\cdots&0&0\\
        0&0&\sin{m\theta_2}&\cos{m\theta_2}&\cdots&0&0 \\
        \vdots&\vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\
        0&0&0&0&\cdots&\cos{m\theta_{D/2}}& -\sin{m\theta_{D/2}}\\
        0&0&0&0&\cdots&\sin{m\theta_{D/2}}&\cos{m\theta_{D/2}}
      \end{pmatrix}
    \]
  }
  \vspace{0.1cm}

  Модификации: NTK-aware RoPE, YaRN...
\end{frame}

\begin{frame}{ALiBi}
  \begin{itemize}
    \item Attention with Linear Biases (ALiBi)
  \end{itemize}
  \begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{img/alibi.pdf}
  \end{figure}
  $m$ --- нетренируемый гиперпараметр, свой для каждой головы в MHA блоке
\end{frame}

\begin{frame}{Трансформерный слой}
  Схема одного слоя трансформера (в изначальном виде):
  \begin{figure}
    \centering
    \includegraphics[width=\linewidth]{img/tlayer_default.png}
  \end{figure}

  В матричной форме:
  \begin{gather*}
    X_{\mathrm{in}} \mapsto X_{\mathrm{Sa}} = X_{\mathrm{in}} +
    \mathrm{MhSa}(X_{\mathrm{in}})\\
    X_{\mathrm{Sa}}\mapsto X_{\mathrm{LN}} =
    \mathrm{LayerNorm}(X_{\mathrm{Sa}})\\
    x_i^{\mathrm{LN}} \mapsto x_i^{\mathrm{MLP}} = x_i^{\mathrm{LN}}
    + \mathrm{MLP}(x_i^{\mathrm{LN}}),\quad i=1,\, 2,\, \dots,\, N\\
    X_{\mathrm{out}} = \mathrm{LayerNorm}(X_{\mathrm{MLP}})
  \end{gather*}
\end{frame}

\begin{frame}{Layer Normalization}
  На входе --- набор векторов $x_i$ (напр. эмбеддинги)

  \medskip

  Для каждого $x_i$ независимо проводится следующая нормализация:
  \begin{gather*}
    \mu_i = \overline{x}_i,\quad \sigma_i^2 = \widehat{D}(x_i)\\
    x_i \mapsto y = \frac{x_i - \mu_i}{\sqrt{\sigma_i^2 +
    \varepsilon}} \mapsto z = \gamma_i y + \beta_i
  \end{gather*}
  Параметры $\gamma_i,\, \beta_i$ обучаемые

  \bigskip

  Позже было показано, что применение слоя LayerNorm перед attention-
  и MLP-блоками даёт более стабильные результаты
\end{frame}

\begin{frame}{Multi-Layer Perceptron (MLP)}
  \begin{itemize}
    \item Блок с обычной полносвязной сетью
    \item Размер входа и выхода должен совпадать (для Residual Connection)
    \item Размеры скрытых слоёв обычно в несколько (2-4) раз больше
      размеров входов
    \item Обычно используется всего 1 скрытый слой (Feed Forward)
    \item Активация обычно $\mathrm{GELU}(x) = x\Phi(x)$
  \end{itemize}
  \begin{figure}
    \includegraphics[width=0.9\linewidth]{img/gelu.pdf}
  \end{figure}
\end{frame}

\begin{frame}{Трансформерная модель-энкодер}
  \begin{tikzpicture}[
      >=Stealth,
      node/.style={draw, rectangle, minimum width=2cm, minimum
      height=1cm, align=center},
      process/.style={draw, rectangle, rounded corners, minimum
      height=1cm, minimum width=2cm, align=center}
    ]

    % Nodes for input
    \node (input) [process, rotate=270] at(0, 0) {Tokenization};
    \node (embedding) [process, rotate=270] at(1.5, 0) {Embedding};

    % Nodes for input tokens
    \node (x1) at (3, 2)  {$x_1$};
    \node (x2) at (3, 1)  {$x_2$};
    \node (x3) at (3, 0)  {$x_3$};
    \node (xn) at (3, -2) {$x_n$};
    \node      at (3, -0.9) {$\cdots$};

    \node (transformer1) [process, rotate=270] at (5, 0) {Transformer
    layer \#1};
    \node (transformerDots) at (6.5, 0) {$\cdots$};
    \node (transformerK) [process, rotate=270] at (8, 0) {Transformer
    layer \#$K$};

    \node (output) [process, rotate=270] at (9.5, 0) {Output};

    \draw[->] (input) -- (embedding);
    \draw[->] (embedding) -- (x1);
    \draw[->] (embedding) -- (x2);
    \draw[->] (embedding) -- (x3);
    \draw[->] (embedding) -- (xn);
    \draw[->, dashed] (x1) -- (transformer1);
    \draw[->, dashed] (x2) -- (transformer1);
    \draw[->, dashed] (x3) -- (transformer1);
    \draw[->, dashed] (xn) -- (transformer1);
    \draw[->] (transformer1) -- (transformerDots);
    \draw[->] (transformerDots) -- (transformerK);
    \draw[->] (transformerK) -- (output);
  \end{tikzpicture}

  \medskip

  Применение: классификация текстов (выход --- вероятности классов),
  угадывание замаскированных токенов в предложении (выход ---
  вероятности токенов, пример --- BERT)
\end{frame}

\begin{frame}{BERT}
  \begin{itemize}
    \item Величина словаря: $M=30k$ токенов
    \item Длина контекста: $N=512$ токенов
    \item Размерность эмбеддинга: $D=1024$
    \item Размерности $K,\, V,\, Q:\: 64$
    \item Количество голов в MHA: $H = 16$
    \item Количество трансформерных слоёв: $K = 24$
    \item Размерность MLP: $D_{\mathrm{MLP}}=4096$
    \item Общее количество параметров: $340 \cdot 10^{6}$
  \end{itemize}
\end{frame}

\begin{frame}{Трансформерная модель-декодер}
  Та же структура, что и у энкодера, но таргеты --- следующий токен в
  последовательности

  \medskip

  Так как задача --- предсказывать следующий токен, то нельзя
  заглядывать вперёд (авторегрессионная модель, вероятность каждого
  токена должна зависеть только от предыдущих)

  \medskip

  Но в self-attention для каждого токена учитываются и все последующие токены

  \medskip

  Решение: зафиксировать $a(x_m, x_n) = 0$ для $m < n$

  То же самое:
  \begin{gather*}
    \mathrm{MaskedSa}(X) = V \cdot \mathrm{Softmax}(K^\top Q + M),\\
    M(m, n) =
    \left\{
      \begin{array}{rl}
        0, &  m \geqslant n\\
        -\infty, & m < n
      \end{array}
      \right.
    \end{gather*}
  \end{frame}

  \begin{frame}{GPT}
    Для GPT-3:

    \begin{itemize}
      \item Величина словаря: неизвестно
      \item Длина контекста: $N=2048$ токенов
      \item Размерность эмбеддинга: $D=12288$
      \item Размерности $K,\, V,\, Q:\: 128$
      \item Количество голов в MHA: $H = 96$
      \item Количество трансформерных слоёв: $K = 96$
      \item Размерность MLP: неизвестно
      \item Общее количество параметров: $175 \cdot 10^{9}$
    \end{itemize}
  \end{frame}

  \begin{frame}{Трансформенная модель Seq2Seq (энкодер-декодер)}
    Эмбеддинг входной последовательность подаётся вход в сеть
    энкодер-трансформеров

    \smallskip

    Эмбеддинг целевой (target) последовательности подаётся на вход в
    сеть модифицированных декодер-трансформеров (появляется
    cross-attention (о нём позже))

    \medskip

    Схема обновлённого слоя декодер-трансформера:
    \begin{figure}
      \centering
      \includegraphics[width=\linewidth]{img/tlayer_decod2.png}
    \end{figure}

  \end{frame}

  \begin{frame}{Cross-Attention}
    Для подсчёта ключей и значений используются выходы энкодер-трансформера
    \begin{gather*}
      K = W_k X_{\mathrm{enc}} + \beta_k\\
      V = W_v X_{\mathrm{enc}} + \beta_v
    \end{gather*}

    Для запросов используются (преобразованные) входы декодер-трансформера
    \[
      Q = W_q X_{\mathrm{dec}} + \beta_q
    \]

    В остальном  всё то же, что и для обычного Self-Attention
    \[
      \mathrm{CA}(X) = V \cdot \mathrm{Softmax}(K^\top Q)
    \]
  \end{frame}

  \begin{frame}{Итоговая Схема Трансформер-Модели}
    \begin{figure}
      \centering
      \includegraphics[height=220pt]{img/tmodel_full.png}
    \end{figure}
  \end{frame}

  \begin{frame}{Проблема квадратичности Self-Attention}
    Честный подсчёт SA имеет временную трудоёмкость $O(N^2)$, где $N$
    --- количество токенов в последовательности

    \vspace{0.4cm}

    Варианты оптимизации:
    \begin{itemize}
      \item Параллелизм и кэширование матриц $K$ и $V$
        \begin{figure}
          \centering
          \includegraphics[width=0.5\linewidth]{img/kv_cache.png}
        \end{figure}
      \item Использование приближений с меньшей трудоёмкостью
    \end{itemize}
  \end{frame}

  \begin{frame}{Sliding Window Attention (Longformer)}
    Считаем SA по некоторому окну\dots
    \begin{figure}
      \centering
      \includegraphics[width=\linewidth]{img/longformer.png}
    \end{figure}

    \vspace{0.4cm}

    Эффективность зависисит от окна
  \end{frame}

  \begin{frame}{Linear Attention --- Linformer}
    Переобозначим $Q,\, K,\, V$ --- входные эмбеддинги, тогда SA считается как:
    \[
      \mathrm{Sa}(QW_i^Q, KW_i^K, VW_i^V)=
      \underbrace{\mathrm{Softmax}\left[\frac{QW_i^Q(KW_i^K)^T}{\sqrt{d_k}}\right]}_{P}VW_i^V,
    \]
    где $W_i^{*}$ --- обучаемые матрицы

    \vspace{0.2cm}

    Можно доказать, что матрица $P$ хорошо приближается меньшими рангами

    \vspace{0.2cm}

    Но низкоранговое приближение $P$ сичтается долго

    Зато быстро можно спроектировать $K$ и $V$
  \end{frame}
  \begin{frame}{Linear Attention --- Linformer}
    \begin{align*}
      &\mathrm{Sa}(QW_i^Q, E_iKW_i^K,
      F_iVW_i^V)\\
      &=\underbrace{\mathrm{Softmax}\left(\frac{QW_i^Q(E_iKW_i^K)^T}{\sqrt{d_k}}\right)}_{\bar{P}}\cdot
      F_iVW_i^V
    \end{align*}
  \end{frame}

  \begin{frame}{Linear Attention --- Linear Transformer}
    \begin{figure}
      \includegraphics[width=\linewidth]{img/linear_transformer.png}
    \end{figure}
  \end{frame}
  \end{document}
